{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiUSW08Srqmu"
      },
      "source": [
        "Warning:\n",
        "* You can skip this notebook if you want to run code on your GPU card or server.\n",
        "\n",
        "Requirement:\n",
        "* [Open this notebook with Google Colab](https://colab.research.google.com/)\n",
        "* Set a GPU environnement by clicking on connect on the corner of the notebook, below \"Comment\" button."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZoAPQ08M_fL"
      },
      "source": [
        "# GPU acceleration\n",
        "\n",
        "If this cell fails, you may want to switch to a GPU-accelerated environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQrhzCIpMmKC",
        "outputId": "ce0e253f-1207-40ee-dbec-8bc1ace2b897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jun 12 13:56:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmAZdNu4EIYO"
      },
      "source": [
        "# Install Ollama and mount the models folder on you Gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHvemduOEFjO",
        "outputId": "c5d42599-5afe-48e1-8629-1d78554c5476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n",
            "100 10941    0 10941    0     0  34449      0 --:--:-- --:--:-- --:--:-- 34514\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and install the Ollama client\n",
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8y3Rwl_uFl4"
      },
      "source": [
        "# Download a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVOui-utcsV"
      },
      "source": [
        "Be careful to have enought space in your Google Drive to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5E8x4YltRVQ",
        "outputId": "22898353-6c62-40b2-ebf6-49fd1615c326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ],
      "source": [
        "!ollama pull llama3:8b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kopv0TEuWV"
      },
      "source": [
        "# Run Ollama in a separate thread and pipe it to ngrok\n",
        "\n",
        "Install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xk7uUfN8AvoH"
      },
      "outputs": [],
      "source": [
        "# ngrok will help setting up a HTTP tunnel to the Ollama service that will run within this Notebook.\n",
        "%pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV7LjvfjA4Kl"
      },
      "source": [
        "\n",
        "The port 11434 used by Ollama is blocked by GColab, only the default HTTP(S) and SSH ports seem to be open.\n",
        "\n",
        "\n",
        "We can easily dodge Google's firewall by running Ollama on a standard port, for instance port 80.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tjeRDIWAGDPN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PORT = 80\n",
        "os.environ['OLLAMA_HOST'] = f\"0.0.0.0:{PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-KRQeZ4HjSX"
      },
      "source": [
        "Define a runner that will pipe Ollama to ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E1b5InvOHc4A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEUg5s7LHBcA"
      },
      "source": [
        "We want to run Ollama in a separate thread so it won't block execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KRCrfXcsEtvo"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    # Ollama serve starts a new Ollama service that will live within the current execution environment.\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmecLsoIclX"
      },
      "source": [
        "Strat Ollama !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIAMJXSoIf-M",
        "outputId": "d6951cfb-fe37-4889-d1d7-10ec281d8b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting"
          ]
        }
      ],
      "source": [
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2dUgJ7QILaz"
      },
      "source": [
        "# Open a ngrok tunnel to our Ollama service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6HZmNHfJqQX",
        "outputId": "ebcb98ee-162d-4d47-a615-4077cba02b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ollama serve\n",
            "2024/06/12 13:56:43 routes.go:1011: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS: OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\n",
            "time=2024-06-12T13:56:43.447Z level=INFO source=images.go:740 msg=\"total blobs: 5\"\n",
            "time=2024-06-12T13:56:43.448Z level=INFO source=images.go:747 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-06-12T13:56:43.448Z level=INFO source=routes.go:1057 msg=\"Listening on [::]:80 (version 0.1.43)\"\n",
            "time=2024-06-12T13:56:43.449Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2721433971/runners\n",
            " * ngrok tunnel to Ollama \"https://d221-34-34-68-238.ngrok-free.app\" -> \"http://127.0.0.1:80/\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Get your ngrok token from your ngrok account:\n",
        "# https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "token=\"2h6NAXyk22ozcuWpFc04PuHpnpu_36hVDAu8zfbvXWbuCkAno\"\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# Start an HTTP tunnel on the specified port\n",
        "public_url = ngrok.connect(PORT).public_url\n",
        "\n",
        "# Print the public URL of the tunnel\n",
        "# Your Ollama service is publicly available at this URL !\n",
        "print(\" * ngrok tunnel to Ollama \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, PORT))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XEt9WvTQDrO"
      },
      "source": [
        "# Load and query a LLM from any remote (like, your laptop)\n",
        "\n",
        "Copy and execute the following on the remote to access our Ollama service !\n",
        "Adapt to your needs ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BjObpT5QtOq",
        "outputId": "fe949b52-73a6-4584-f635-a2d0648dfbae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OLLAMA_HOST=https://d221-34-34-68-238.ngrok-free.app ollama run llama3:8b\n"
          ]
        }
      ],
      "source": [
        "print(\"OLLAMA_HOST={} ollama run llama3:8b\".format(public_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb8jHpaSu9FL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi7Ul9wLRKKy"
      },
      "source": [
        "**Current limitations**\n",
        "- Google Colab resources are quite limited, with the free plan any model larger that 20Gb will probably fail to load.\n",
        "- Your Ollama server is tied to the execution environnment. You will have to download & run the model again after every environment reset...\n",
        "- In my (few) experiments I had the feeling that GColab was disconnecting more frequently than the usual. Maybe there is some kind of traffic limitation to prevent using a GColab as a Web server ?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
