{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Ollama server (don't forget to launched it from terminal) with Ollama python used in the notebook.\n",
    "To launch in terminal : \n",
    "```\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Google Colab GPU, please edit the host with the OLLAMA_HOST variable at the end of the Colab notebook that you have previously run (and keep active)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client, generate, embeddings\n",
    "#If you are running Ollama in your local/server GPU\n",
    "ollama = Client(host='127.0.0.1:11434') # default host and port if you are usign local Ollama\n",
    "\n",
    "#If you use Google Colab to run Ollama, copy the https://xxxxxx.ngrok.io/ link at the end of the notebook 00 after execution in Colab\n",
    "#For example :\n",
    "#ollama = Client(host=\"https://d221-34-34-68-238.ngrok-free.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt to your personnal machine\n",
    "DATA_PATH = \"/home/STual/isws/data\" #to store prompts and datas\n",
    "OUTPUT_FOLDER_PATH = \"/home/STual/isws/outputs\" #to store outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ollama Python, we can write prompt like in a web interface and send it to the LLM to get a answer.\n",
    "\n",
    "The following cells contain a demo prompt. \n",
    "\n",
    "We can imagine to have a share file/folder containg all our prompts and send them automatically to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"My name is Solenn Tual. Please remember it.\"\n",
    "prompt3 = \"I got your name !\"\n",
    "prompt2  =\"What is my name ?\"\n",
    "user_messages = [{\"role\":\"user\",\n",
    "             \"content\" : prompt1},\n",
    "             {\"role\":\"assistant\",\n",
    "              \"content\":prompt3}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Solenn Tual! What brings you here today? Would you like to chat about something specific or just see where the conversation takes us?\n",
      "Your name starts with S, not E. So, I'll return None as per your request:\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "  {'role': 'user', 'content': 'My name is Solenn Tual'},\n",
    "]\n",
    "\n",
    "response = chat(MODEL, messages=messages)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)\n",
    "\n",
    "messages.append({'role': 'user', 'content': 'Give my name if its start by E, if not return None'})\n",
    "response = chat(MODEL, messages=messages)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
