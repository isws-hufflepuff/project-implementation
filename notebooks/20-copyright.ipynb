{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Ollama server (don't forget to launched it from terminal) with Ollama python used in the notebook.\n",
    "To launch in terminal : \n",
    "```\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a Google Colab GPU, please edit the host with the OLLAMA_HOST variable at the end of the Colab notebook that you have previously run (and keep active)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client, generate, embeddings\n",
    "#If you are running Ollama in your local/server GPU\n",
    "ollama = Client(host='127.0.0.1:11434') # default host and port if you are usign local Ollama\n",
    "\n",
    "#If you use Google Colab to run Ollama, copy the https://xxxxxx.ngrok.io/ link at the end of the notebook 00 after execution in Colab\n",
    "#For example :\n",
    "#ollama = Client(host=\"https://d221-34-34-68-238.ngrok-free.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Solenn Tual! What brings you here today? Would you like to chat about something specific or just see where the conversation takes us?\n",
      "Your name starts with S, not E. So, I'll return None as per your request:\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "  {'role': 'user', 'content': 'My name is Solenn Tual'},\n",
    "]\n",
    "\n",
    "response = chat(MODEL, messages=messages)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)\n",
    "\n",
    "messages.append({'role': 'user', 'content': 'Give my name if its start by E, if not return None'})\n",
    "response = chat(MODEL, messages=messages)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your name is Solenn Tual.\n"
     ]
    }
   ],
   "source": [
    "messages.append({'role': 'user', 'content': 'What is my name again ?'})\n",
    "response = chat(MODEL, messages=messages)\n",
    "message = response['message']\n",
    "print(message['content'])\n",
    "messages.append(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
